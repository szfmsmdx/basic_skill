{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实现线性层的 Lora 微调"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoraLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, rank, merge, alpha, dropout=0.1):\n",
    "        \"\"\"\n",
    "        LoraLinear: Lora线性层,代替原始的线性层\n",
    "        \"\"\"\n",
    "        # merge: 是否加载 Lora 权重\n",
    "        # alpha: Lora 权重系数\n",
    "        # rank: LoRA低秩的维度\n",
    "        super(LoraLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.rank = rank\n",
    "        self.merge = merge\n",
    "        self.alpha = alpha\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # TODO: 构建Lora主体\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        if self.rank > 0:\n",
    "            self.weight_b = nn.Parameter(torch.zeros(out_features, self.rank))\n",
    "            self.weight_a = nn.Parameter(torch.zeros(self.rank, in_features))    # 后面再初始化成正态分布\n",
    "            self.scale = self.alpha / self.rank\n",
    "            self.linear.weight.requires_grad = False\n",
    "\n",
    "        if self.dropout > 0:\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "        else:\n",
    "            self.dropout = nn.Identity()   # 恒等映射\n",
    "\n",
    "        # TODO: 初始化权重\n",
    "        nn.init.kaiming_uniform_(self.weight_a, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.weight_b)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.rank > 0 and self.merge:\n",
    "            output = F.linear(x, self.linear.weight + self.scale * self.weight_b @ self.weight_a, self.linear.bias) # linear函数是y = xA^T + b\n",
    "            output = self.dropout(output)\n",
    "        else:\n",
    "            output = self.dropout(self.linear(x))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 替换手写 Transformer 模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Transformer\n",
    "\n",
    "enc_voc_size = 6000\n",
    "dec_voc_size = 8000\n",
    "src_pad_idx = 1\n",
    "tgt_pad_idx = 1\n",
    "tgt_sos_idx = 2\n",
    "batch_size = 32\n",
    "max_len = 1024\n",
    "d_model = 512\n",
    "n_layers = 3\n",
    "n_head = 2\n",
    "ffn_hidden = 1024\n",
    "drop_prob = 0.1\n",
    "device = \"cpu\"\n",
    "\n",
    "model = Transformer(\n",
    "    src_pad_idx=src_pad_idx,\n",
    "    tgt_pad_idx=tgt_pad_idx,\n",
    "    enc_voc_size=enc_voc_size,\n",
    "    dec_voc_size=dec_voc_size,\n",
    "    max_len=max_len,\n",
    "    d_model=d_model,\n",
    "    n_layers=n_layers,\n",
    "    n_head=n_head,\n",
    "    ffn_hidden=ffn_hidden,\n",
    "    drop_prob=drop_prob\n",
    ").to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 用LoraLinear替换原始的Linear\n",
    "# 收集需要替换的模块信息\n",
    "modules_to_replace = []\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, nn.Linear):\n",
    "        in_features = module.in_features\n",
    "        out_features = module.out_features\n",
    "        rank = 32\n",
    "        merge = True\n",
    "        alpha = 0.5\n",
    "        new_module = LoraLinear(in_features, out_features, rank, merge, alpha)\n",
    "        new_module.linear.weight = module.weight  # 替换原始的权重\n",
    "        new_module.linear.bias = module.bias      # 如果有偏置，也需要替换\n",
    "        modules_to_replace.append((name, new_module))\n",
    "\n",
    "# 替换模块\n",
    "for name, new_module in modules_to_replace:\n",
    "    # 使用 _modules 替换模块\n",
    "    parent_module, attr_name = name.rsplit('.', 1)  # 获取父模块和属性名\n",
    "    parent = model\n",
    "    if parent_module:  # 如果有父模块\n",
    "        parent = model.get_submodule(parent_module)\n",
    "    setattr(parent, attr_name, new_module)  # 替换子模块\n",
    "    # print(f\"Replace {name} with LoraLinear\")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PEFT框架"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 540,672 || all params: 494,573,440 || trainable%: 0.1093\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n",
    "import os\n",
    "\n",
    "model_name_or_path = \"Qwen/Qwen2.5-0.5B\"\n",
    "tokenizer_name_or_path = \"Qwen/Qwen2.5-0.5B\"\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name_or_path)\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\pytorch\\Lib\\site-packages\\transformers\\integrations\\sdpa_attention.py:48: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 16.26947593688965\n",
      "Epoch 2/10, Loss: 16.191492080688477\n",
      "Epoch 3/10, Loss: 16.114389419555664\n",
      "Epoch 4/10, Loss: 16.029483795166016\n",
      "Epoch 5/10, Loss: 15.952570915222168\n",
      "Epoch 6/10, Loss: 15.91295051574707\n",
      "Epoch 7/10, Loss: 15.798820495605469\n",
      "Epoch 8/10, Loss: 15.704445838928223\n",
      "Epoch 9/10, Loss: 15.628195762634277\n",
      "Epoch 10/10, Loss: 15.592994689941406\n",
      "Model fine-tuning completed and saved.\n"
     ]
    }
   ],
   "source": [
    "# 进行微调\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 定义一个简单的文本数据集\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", max_length=self.max_length, truncation=True, padding=\"max_length\")\n",
    "        input_ids = inputs[\"input_ids\"].squeeze(0)\n",
    "        attention_mask = inputs[\"attention_mask\"].squeeze(0)\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": input_ids  # For causal LM, labels are the same as input_ids\n",
    "        }\n",
    "\n",
    "# 示例训练数据\n",
    "train_texts = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"I am fine, thank you!\",\n",
    "    \"What are you doing today?\",\n",
    "    \"I am learning about LoRA and transformers.\",\n",
    "    # Add more training samples here\n",
    "]\n",
    "\n",
    "# 创建数据集和数据加载器\n",
    "train_dataset = TextDataset(train_texts, tokenizer)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# 设置训练参数\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "num_epochs = 10\n",
    "\n",
    "# 训练循环\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "# 保存微调后的模型\n",
    "model.save_pretrained(\"finetuned_model\")\n",
    "tokenizer.save_pretrained(\"finetuned_model\")\n",
    "\n",
    "print(\"Model fine-tuning completed and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合并模型\n",
    "from peft import merge_lora_weights, LoraConfig, TaskType\n",
    "\n",
    "model_name_or_path = \"Qwen/Qwen2.5-0.5B\"\n",
    "tokenizer_name_or_path = \"Qwen/Qwen2.5-0.5B\"\n",
    "model_path = \"finetuned_model\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name_or_path)\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.merge_lora_weights()\n",
    "model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
